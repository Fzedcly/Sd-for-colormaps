# -*- coding: utf-8 -*-
"""
LoRA è®­ç»ƒè„šæœ¬ (é€‚é… diffusers 0.35+ / PEFT)ï¼Œå·²ä¿®å¤:
- fp16 è®­ç»ƒä¸ç¨³å®š -> ä½¿ç”¨ AMP + bfloat16 (å¯å›é€€) + é™ LR + æ¢¯åº¦è£å‰ªï¼Œé¿å… NaN
- æ–°ç‰ˆ save_lora_weights éœ€æ˜¾å¼ä¼ å±‚ -> ç›´æ¥å¯¼å‡º LoRA å‚æ•°ä¸º safetensors
"""

import os
from pathlib import Path

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import torchvision.transforms as T

from diffusers import StableDiffusionPipeline, DDPMScheduler
from peft import LoraConfig
from safetensors.torch import save_file

# ================= é…ç½® =================
MODEL_ID = "runwayml/stable-diffusion-v1-5"

DATA_DIR = "./datasets/colormaps"     # æŒ‰ä½ çš„ç›®å½•è°ƒæ•´
OUT_DIR  = "./lora_colormap_output"

RES    = 512
BATCH  = 1
EPOCHS = 3
LR     = 5e-5            # é™ä¸€ç‚¹ï¼Œé…åˆ AMP æ›´ç¨³
RANK   = 4
ALPHA  = RANK
SCALE  = 0.18215         # SD1.x VAE latent ç¼©æ”¾

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# ä¼˜å…ˆ bfloat16ï¼ˆAda æ”¯æŒï¼‰ï¼Œå¦åˆ™ fp16ï¼ŒCPU ç”¨ fp32
if DEVICE == "cuda" and torch.cuda.is_bf16_supported():
    AMP_DTYPE = torch.bfloat16
elif DEVICE == "cuda":
    AMP_DTYPE = torch.float16
else:
    AMP_DTYPE = torch.float32

torch.backends.cuda.matmul.allow_tf32 = True
if hasattr(torch, "set_float32_matmul_precision"):
    torch.set_float32_matmul_precision("high")


# ================= æ•°æ®é›† =================
class ColormapDS(Dataset):
    def __init__(self, root):
        self.items = []
        root = Path(root)
        self.tfm = T.Compose([
            T.Resize((RES, RES), interpolation=T.InterpolationMode.BILINEAR),
            T.ToTensor(),
            T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
        ])
        for d in sorted(root.iterdir()):
            if not d.is_dir():
                continue
            img = d / "grad1.png"
            prf = d / "prompt.txt"
            if img.exists() and prf.exists():
                self.items.append((img, prf.read_text(encoding="utf-8").strip()))
        if not self.items:
            raise RuntimeError(f"æ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼š{root}")

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        p, prompt = self.items[idx]
        img = Image.open(p).convert("RGB")
        return {"pixel": self.tfm(img), "prompt": prompt}


def extract_lora_state_dict(model: torch.nn.Module):
    """
    ä»…å¯¼å‡º LoRA ç›¸å…³å‚æ•°ï¼ˆåå­—é‡ŒåŒ…å« 'lora' çš„æƒé‡ï¼‰ï¼Œç”¨äº safetensors ä¿å­˜ã€‚
    é€‚é… PEFT + diffusers æ–°ç‰ˆçš„ adapter æ³¨å…¥ã€‚
    """
    sd = model.state_dict()
    out = {k: v.detach().cpu() for k, v in sd.items() if "lora" in k.lower()}
    if not out:
        raise RuntimeError("æœªæ‰¾åˆ° LoRA å‚æ•°ï¼Œç¡®è®¤ add_adapter æ˜¯å¦æˆåŠŸä»¥åŠ target_modules æ˜¯å¦åŒ¹é…ã€‚")
    return out


def main():
    ds = ColormapDS(DATA_DIR)
    dl = DataLoader(
        ds,
        batch_size=BATCH,
        shuffle=True,
        num_workers=2 if DEVICE == "cuda" else 0,
        pin_memory=(DEVICE == "cuda"),
    )

    # ================= ç®¡çº¿ =================
    pipe = StableDiffusionPipeline.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float32,      # ä¸»æ¨¡å‹æƒé‡ä¿ç•™ fp32ï¼Œæ›´ç¨³
        safety_checker=None,
    )
    pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)
    pipe.to(DEVICE)

    unet = pipe.unet
    vae  = pipe.vae
    text_encoder = pipe.text_encoder
    tokenizer    = pipe.tokenizer

    # å†»ç»“é LoRA æƒé‡
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    unet.requires_grad_(False)

    # çœæ˜¾å­˜
    if hasattr(unet, "enable_gradient_checkpointing"):
        unet.enable_gradient_checkpointing()

    # ================= ç”¨ PEFT ç»™ U-Net åŠ  LoRAï¼ˆæ–°ç‰ˆæ¨èåšæ³•ï¼‰ =================
    unet_lora_cfg = LoraConfig(
        r=RANK,
        lora_alpha=ALPHA,
        init_lora_weights="gaussian",
        target_modules=["to_q", "to_k", "to_v", "to_out.0"],  # SD1.x CrossAttention åç§°
    )
    unet.add_adapter(unet_lora_cfg)

    # åªè®­ç»ƒ LoRA å‚æ•°
    train_params = [p for p in unet.parameters() if p.requires_grad]
    opt = torch.optim.AdamW(train_params, lr=LR, betas=(0.9, 0.999), weight_decay=1e-4)

    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == "cuda" and AMP_DTYPE != torch.float32))

    num_train_steps = len(dl) * EPOCHS
    print(f"æ ·æœ¬æ•°={len(ds)}  æ€»æ­¥æ•°â‰ˆ{num_train_steps}  è®¾å¤‡={DEVICE}  amp_dtype={AMP_DTYPE}")

    # ================= è®­ç»ƒå¾ªç¯ =================
    for ep in range(EPOCHS):
        for step, batch in enumerate(dl, 1):
            imgs = batch["pixel"].to(DEVICE, dtype=torch.float32)  # é€å…¥ VAE å‰ä¿æŒ fp32

            # å›¾åƒ -> VAE latent
            with torch.no_grad():
                latents = vae.encode(imgs).latent_dist.sample() * SCALE
                latents = latents.to(DEVICE)

            # æ–‡æœ¬ -> ç¼–ç ï¼ˆCLIPï¼‰
            tokens = tokenizer(
                list(batch["prompt"]),
                padding="max_length",
                max_length=tokenizer.model_max_length,
                truncation=True,
                return_tensors="pt",
            )
            with torch.no_grad():
                enc = text_encoder(tokens.input_ids.to(DEVICE))[0]  # last_hidden_state

            noise = torch.randn_like(latents)
            timesteps = torch.randint(
                0, pipe.scheduler.config.num_train_timesteps,
                (latents.shape[0],), device=DEVICE
            ).long()
            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)

            opt.zero_grad(set_to_none=True)

            # AMP å‰å‘ï¼Œä¼˜å…ˆ bfloat16
            with torch.cuda.amp.autocast(enabled=(DEVICE=="cuda"), dtype=AMP_DTYPE):
                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states=enc).sample
                loss = F.mse_loss(model_pred.float(), noise.float())

            # AMP åå‘ & æ›´æ–°
            if DEVICE == "cuda":
                scaler.scale(loss).backward()
                # æ¢¯åº¦è£å‰ªæŠ‘åˆ¶çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(train_params, 1.0)
                scaler.step(opt)
                scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(train_params, 1.0)
                opt.step()

            if step % 10 == 0:
                print(f"[ep {ep+1}/{EPOCHS}] step {step:04d}/{len(dl)}  loss={loss.item():.6f}")

            # è‹¥ä»å‡ºç° NaNï¼Œç«‹åˆ»ä¸­æ–­å¹¶ç»™å‡ºæç¤º
            if torch.isnan(loss):
                raise RuntimeError(
                    "æ£€æµ‹åˆ° loss=NaNï¼šè¯·å°è¯•å°† RES é™åˆ° 384ã€LR å†é™åˆ° 2e-5ï¼Œæˆ–ç¼©å° batchã€‚"
                )

    # ================= ä¿å­˜ LoRA (safetensors) =================
    os.makedirs(OUT_DIR, exist_ok=True)
    unet_lora = extract_lora_state_dict(unet)
    save_path = os.path.join(OUT_DIR, "pytorch_lora_weights.safetensors")
    save_file(unet_lora, save_path)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼ŒLoRA å·²ä¿å­˜åˆ°:", save_path)


if __name__ == "__main__":
    main()
